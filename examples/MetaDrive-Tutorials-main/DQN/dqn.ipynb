{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks (DQN)\n",
    "*Note: This tutorial assumes that you have completed reading the policy gradient notebook and have an understanding of Markov Decision Processes and policy gradient models. If not, please revisit them [here](../PolicyGradient/policygradient_discrete.ipynb) and [here](../PolicyGradient/policygradient_continuous.ipynb).*\n",
    "\n",
    "The goals of this notebook are to:\n",
    "1. Explain what Deep Q-Networks are, and why we might want to use them.\n",
    "2. Understand its mathematical foundations.\n",
    "3. Explain potential issues with DQN.\n",
    "4. Apply it to the Metadrive environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the relationship between Deep Q Networks and Deep Q Learning?\n",
    "\n",
    "In this notebook, we throw around the terms \"Deep Q Networks\" as well as \"Deep Q Learning\" a lot. It's worthwhile to take a moment to understand the difference between the two.\n",
    "* Q-Learning is a reinforcement learning algorithm that's based on learning the value of taking a particular action in a particular state. However, it doesn't require us to use a neural network. We could use a table to store the values of each state-action pair, and update them as we learn more about the environment, although this approach is only feasible for small environments.\n",
    "* Deep Q-Learning is the exact same algorithm, except we use a neural network to represent the Q function. This allows us to scale Q-Learning to large environments, and generalize across similar states.  \n",
    "* A Deep Q Network is the neural network that we use to represent the Q function when using the Deep Q-Learning algorithm. It takes in a state as input, and outputs the Q value for each action.\n",
    "\n",
    "We'll cover all of these in more detail later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Q-Learning?\n",
    "\n",
    "Just like in Policy Gradients, our goal is to find the optimal policy $\\pi^*$ that maximizes the expected return $J(\\pi)$. However, instead of directly learning the policy, we will try to learn the optimal action-value function $Q^*(s,a)$, which is the expected return of taking action $a$ in state $s$ and then following the optimal policy $\\pi^*$.\n",
    "\n",
    "Once we have a good estimate of $Q^*(s,a)$, we can find the optimal policy by simply taking the action with the highest $Q$ value in each state. This is known as the **greedy policy**.\n",
    "\n",
    "#### Tic-Tac-Toe Example\n",
    "Let's illustrate this with an example. Consider the game of Tic-Tac-Toe. The state of the game is the current board configuration, and the actions are the possible moves that the current player can make. The optimal action-value function $Q^*(s,a)$ tells us the expected return of taking action $a$ in state $s$ and then following the optimal policy $\\pi^*$. You can think of it as a sort of \"cheat sheet\" that tells us whether it is possible for us to win the game if we take a particular action in a particular state, or if the position is a forced loss or draw.\n",
    "\n",
    "If we have this cheat sheet, we can easily find the optimal policy. We simply choose actions which have the highest $Q$ value in each state. That is, we take the action that says we can force a win, if it exists. If not, we take the action that says we can force a draw, if it exists.\n",
    "\n",
    "It's clear that using this approach, we'll never lose a game of Tic-Tac-Toe. This is the core idea behind Q-Learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DQN?\n",
    "In the last notebook, you learned about policy gradients - a way to create powerful learners that update on the gradient of the objective function itself.\n",
    "\n",
    "However, they have a few key downsides that are addressed with DQN:\n",
    "* **Sample inefficiency**: to accurately estimate the gradient via Monte-Carlo methods, the model needs to run over many episodes on the environment. Policy-gradient methods typically require many times more training samples than other methods. You probably observed that the REINFORCE algorithm took many episodes just to learn to consistently drive forwards on the Metadrive environment! While this is alleviated with more advanced algorithms like Proximal Policy Optimization (PPO) (which we'll explore later), , Deep-Q learning inherently bypasses the sample inefficiency of policy gradient methods. This is because Q-learning methods can take advantage of off-policy learning and replay buffers to continue to learn from its past mistakes, as the value of a state-action pair stays relatively constant as the agent learns (in the short term), making it feasible to have the same trajectory in the replay buffer for multiple epochs and update on it many times.\n",
    "\n",
    "* **Exploration-exploitation trade-off**: To learn in any environment, an agent will need to explore (to sample from diverse environmental states) and exploit (focus on high-reward strategies). To improve at chess, we might play our best moves most of the time, but also throw in a few questionable moves to test them out and broaden our strategic choices. Policy gradient methods can get stuck in local optimal due to insufficient exploration. In Deep-Q learning, we explicitly set a parameter that sets the balance between exploration and exploitation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $V(s)$ and $Q(s, a)$\n",
    "As we dive into our exploration of Q-networks, it's useful to take a look at a key idea that will come up often later on: we can denote the *value* of a state (how good a position is) by denoting **value**, $V(s)$ and **quality**, $Q(s, a)$ functions. \n",
    "\n",
    "More specifically, the **value** function represents the value of a **state** while the **quality** function represents the quality of a **state-action pair**. OpenAI's [Spinning Up on Deep RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#trajectories) has a great article on this, which we'll reiterate below:\n",
    "\n",
    "> There are four main functions of note:\n",
    ">\n",
    ">The On-Policy Value Function, $V^{\\pi}(s)$, which gives the expected return if you start in state s and always act according to policy $\\pi$:\n",
    ">\n",
    ">$$V^{\\pi}(s) = \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s\\right.}]$$\n",
    ">\n",
    ">The On-Policy Action-Value Function, $Q^{\\pi}(s,a)$, which gives the expected return if you start in state s, take an arbitrary action a (which may not have come from the policy), and then forever after act according to policy $\\pi$:\n",
    ">\n",
    ">$$Q^{\\pi}(s,a) = \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}]$$\n",
    ">\n",
    ">The Optimal Value Function, $V^*(s)$, which gives the expected return if you start in state s and always act according to the optimal policy in the environment:\n",
    ">\n",
    ">$$V^*(s) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s\\right.}]$$\n",
    ">\n",
    ">The Optimal Action-Value Function, $Q^*(s,a)$, which gives the expected return if you start in state s, take an arbitrary action a, and then forever after act according to the optimal policy in the environment:\n",
    ">\n",
    ">$$Q^*(s,a) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note is that the value of a state or the quality of a state-action pair can be very different depending on our policy. For example, in chess, a mate-in-five position that's down on material would be of great value to a grandmaster but useless to a novice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning $Q(s, a)$ into $\\pi(s, a)$\n",
    "\n",
    "Given the quality function $Q^\\pi(s_t, a_t)$, there are a variety of strategies we can use to select our actions. One is always taking the action that we believe has the highest quality:\n",
    "$$\n",
    "\\pi(s_t, a_t) = \\underset{a \\in A}{\\operatorname{argmax}} \\space Q^\\pi(s_t, a)\n",
    "$$ \n",
    "\n",
    "If we've learned the optimal quality function $Q^*(s, a)$, then this works, but while we're training, we want a way to explore. Take a look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(8, 16, 'end')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "red, green, light_green, black = np.zeros((4, 32, 32, 3))\n",
    "red += np.array([1, 0.5, 0.5])\n",
    "green += np.array([0, 0.75, 0])\n",
    "light_green += np.array([0.5,1,0.5])\n",
    "plt.imshow(np.concatenate([black, light_green, black, red, green, black], axis=1))\n",
    "plt.text(42, 16, '+2', color='white', fontsize=14)\n",
    "plt.text(108, 16, '-2', color='white', fontsize=14)\n",
    "plt.text(134, 16, '+10', color='white', fontsize=14)\n",
    "plt.text(72, 16, 'start', color='white', fontsize=14)\n",
    "plt.text(168, 16, 'end', color='white', fontsize=14)\n",
    "plt.text(8, 16, 'end', color='white', fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we gather any samples about the environment, the model is equally likely to explore either the +2 square and the -2 square. Let's imagine it explores the +2 square before ending the episode; after updating at the end of the episode, the model learns to always seek out the +2 square, never being able to reach the +10 square (with each update afterward only confirming its bias). Clearly, we need to add a term for exploration:\n",
    "\n",
    "### Epsilon-Greedy\n",
    "\n",
    "We introduce a parameter epsilon ($\\epsilon$) that controls the balance between exploration and exploitation. We can then define our policy as follows:\n",
    "\n",
    "$$\\pi(s) = \\begin{cases}\n",
    "\\underset{a \\in A}{\\operatorname{argmax}} \\space Q(s, a) & \\text{with probability } 1 - \\epsilon \\\\\n",
    "\\text{random action} & \\text{with probability } \\epsilon\n",
    "\\end{cases}$$\n",
    "\n",
    "We set $\\epsilon$ to a high value (~0.3) at the start of training and slowly reduce it as the model explores more strategies.\n",
    "You can retrieve the greedy policy by setting $\\epsilon$ to 0. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning $Q(s, a)$\n",
    "\n",
    "It's clear that if we have a good estimate of $Q^*(s, a)$, we can find the optimal policy by simply taking the action with the highest $Q$ value in each state. However, how do we actually learn $Q^*(s, a)$?\n",
    "\n",
    "It's simple:\n",
    "1. We start off with a random neural network that takes in a state as input, and outputs the Q value for each action. We denote this neural network $Q_\\phi(s, a)$, where $\\phi$ represents the parameters of the neural network.\n",
    "2. We collect a bunch of trajectories by running an epsilon-greedy policy on the environment.\n",
    "3. For each state-action pair in each trajectory:\n",
    "    * Our current estimate of $Q^*(s,a)$, $Q_{\\phi}(s, a)$, already outputs what it thinks the best possible return from that state-action pair is. \n",
    "    * However, as we explore, we might find that the actual best possible return from that state-action pair is different from what our current estimate says.\n",
    "    * We can use the Bellman equation to update our current estimate of $Q^*(s,a)$ to be closer to the actual best possible return.\n",
    "4. We repeat steps 2 and 3 until our estimate of $Q^*(s,a)$ converges. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "The **Bellman Equation** is a recursive rewriting of the Q-value equation, and the formula that we'll be using to update our Q-model:\n",
    "$$\n",
    "Q^\\pi(s_t, a_t) = E_\\pi[r_{t+1}] + \\gamma  Q^\\pi(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "Essentially, the Bellman equation states that the Q-value of a state-action pair is equal to the immediate reward of taking that action, plus the discounted Q-value of the next state-action pair.\n",
    "\n",
    "This equation is pretty intuitive, but we can more rigorously derive the above formula from our definition of $Q^\\pi(s_t, a_t)$. If you enjoy math, read on:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the Bellman Equation\n",
    "We start off with the definition of the Q-function from Spinning Up:\n",
    "$$\n",
    "Q^\\pi(s_t, a_t) = E_\\pi\\left[R(\\tau) \\left| s_0 = s_t, a_0 = a_t\\right.\\right]\n",
    "$$\n",
    "We expand the definition of $R(\\tau)$:\n",
    "\n",
    "$$\n",
    "Q^\\pi(s_t, a_t) = E_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k  r_{t+k+1} | s = s_t, a = a_t\\right]\n",
    "$$\n",
    "We expand the summation:\n",
    "$$\n",
    "= E_\\pi\\left[r_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k  r_{t+k+2} | s = s_{t}, a = a_{t}\\right]\n",
    "$$\n",
    "As expectation is distributive over addition:\n",
    "$$ = E_\\pi[r_{t+1}] + E_\\pi\\left[\\gamma \\sum_{k=0}^\\infty \\gamma^k  r_{t+k+2} | s = s_{t+1}, a = a_{t+1}\\right]$$\n",
    "\n",
    "Using the definition of expectation:\n",
    "$$\n",
    "= \\left( \\sum_{a \\in A} Pr_\\pi(a | s_t)  \\sum_{s' \\in S} Pr(s'|s_t, a)\\right)  r(s') + E_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k  r_{t+k+2} | s = s_{t+1}, a = a_{t+1}\\right]\n",
    "$$\n",
    "\n",
    "Recall that $Pr_\\pi(a | s_t)$ is the probability of our current policy taking action $a$ in state $s_t$.\n",
    "Since Q-Learning doesn't explicitly predict the likelihood of the action being taken (unlike Policy Gradient, which does), the value of $Pr_\\pi(a | s_t)$ is unknown - which is why we're updating $Q^\\pi(s, a)$ via Monte-Carlo sampling of the environment.\n",
    "\n",
    "We add the term $Pr(s'|s_t, a)$ to allow for stochastic environments - where taking action A at state S can transition to many possible successor states $s' \\in S$.\n",
    "\n",
    "$$\n",
    "\\left( \\sum_{a \\in A} Pr_\\pi(a | s_t)  \\sum_{s' \\in S} Pr(s'|s_t, a)\\right) r(s') = E[r_{t+1}]\n",
    "$$\n",
    "$$\n",
    "\\gamma E_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k  r_{t+k+2} | s = s_{t+1}, a = a_{t+1}\\right] = \\gamma  Q^\\pi(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^\\pi(s_t, a_t) = E[r_{t+1}] + \\gamma  Q^\\pi(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "Which is the Bellman Equation.\n",
    "\n",
    "Since the optimal policy will always pick the action with the highest Q-value, we observe the following:\n",
    "$$\n",
    "Q^*(s_t, a_t) = E[r_{t+1}] + \\gamma \\, \\underset{a \\in A}{\\operatorname{max}} \\space Q^*(s_{t+1}, a)\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Bellman Equation to Update $Q(s, a)$\n",
    "\n",
    "Let's say our agent is dropped in an environment and we're looking to learn a robust and accurate Q-function. \n",
    "\n",
    "More specifically, we're looking to find $Q^{\\pi^*}(s_t, a_t)$, which is the *optimal quality function* that denotes the quality of each state-action pair under the optimal policy $\\pi^*(a|s)$. We can update our Q-function by doing Monte-Carlo sampling from our environment. Under our policy, we gather trajectories from the environment and update our Q-value function as so:\n",
    "\n",
    "If the current step isn't the last step:\n",
    "$$Q_{\\phi}(s_t, a_t) \\leftarrow \\left(r_{t+1} + \\gamma \\, \\underset{a \\in A}{\\operatorname{max}} \\space Q_{\\phi}(s_{t+1}, a) \\right)$$\n",
    "If the current step is the last step:\n",
    "$$Q_{\\phi}(s_t, a_t) \\leftarrow r_{t+1}$$\n",
    "\n",
    "As we gather more samples, we can optimize Q by using mean squared error or a similar loss metric to minimize the distance between Q and its target."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not use Q-learning?\n",
    "\n",
    "Q-learning has flaws as well:\n",
    "* Lack of stability. The *moving target problem* occurs:\n",
    "    $$Q_{\\phi}(s_t, a_t) \\leftarrow \\left(r_{t+1}  + \\gamma \\, \\underset{a \\in A}{\\operatorname{max}} \\space Q_{\\phi}(s_{t+1}, a) \\right)$$\n",
    "    * Observe how this not only updates $Q_{\\phi}(s_t, a_t)$ but also implicitly updates $\\pi(s_t, a_t)$. When we update our value function, the behavior of our policy shifts, which changes the true value of the moves we're taking. This makes training unstable by default. Double and dueling DQNs alleviate this problem, and we'll talk about them in the next notebook.\n",
    "    * The Q-Value estimate provided by the neural net does not correspond to the actual Q-Value that a greedy function based on $Q_{\\phi}$ would have. Mathematically, $Q_{\\phi}(s, a) \\ne Q^{\\pi_{\\phi}}(s, a)$.\n",
    "* Difficulties with continuous environments: the above update rule also only works in discretized environments, as $ \\underset{a \\in A}{\\operatorname{max}} \\space Q^{\\pi}(s_{t+1}, a)$ is intractable to calculate if there are infinite possible actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You try it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we have two notebooks - one with exercises, the other with solutions. We recommend that you try the first notebook first, and then use the second notebook as a reference if you get stuck.\n",
    "* [DQN Exercises](dqn_exercise.ipynb)\n",
    "* [DQN Solutions](dqn_solution.ipynb)\n",
    "\n",
    "Before completing the second half of the exercises, be sure to check out the explanations behind [Double and Dueling DQNs](double_dqn.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Readings\n",
    "This tutorial was based on a few source documents. If you're interested, you can read the more extensive explanations from the sources below:\n",
    "1. [Value Iteration vs Policy Iteration](https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration)\n",
    "2. [A mathematical introduction to Reinforcement Learning, CMU](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
